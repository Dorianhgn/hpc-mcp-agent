# mcp_hpc_client.py
import os
import json
import uuid
import time
import asyncio
from upstash_redis import Redis
from mcp.server.fastmcp import FastMCP

# Init FastMCP
mcp = FastMCP("HPC-Orchestrator")

# Redis client avec upstash_redis (REST API)
redis_client = Redis.from_env()

QUEUE_NAME = "hpc:jobs"
RESULTS_PREFIX = "hpc:result:"


def submit_job(job_type: str, **params) -> str:
    """Soumet un job dans la queue Redis et attend le r√©sultat"""
    job_id = str(uuid.uuid4())
    
    job = {
        "id": job_id,
        "type": job_type,
        "timestamp": time.time(),
        **params
    }
    
    # Envoie dans la queue
    redis_client.lpush(QUEUE_NAME, json.dumps(job))
    print(f"üì§ Job {job_id[:8]} submitted (type: {job_type})")
    
    # Attend le r√©sultat (avec timeout adaptatif)
    timeout = {
        "podman_build": 600,      # 10 min
        "podman_run": 3600,       # 1h
        "huggingface_check": 30,  # 30s
        "gpu_info": 60,           # 1 min
        "slurm_queue": 30,        # 30s
    }.get(job_type, 300)
    
    result = wait_for_result(job_id, timeout)
    return result


def wait_for_result(job_id: str, timeout: int) -> str:
    """Poll Redis pour r√©cup√©rer le r√©sultat"""
    result_key = f"{RESULTS_PREFIX}{job_id}"
    
    for i in range(timeout):
        result_json = redis_client.get(result_key)
        
        if result_json:
            result = json.loads(result_json)
            
            if result.get("status") == "success":
                return result.get("output", "")
            else:
                error = result.get("error", "Unknown error")
                stderr = result.get("stderr", "")
                return f"‚ùå Job failed:\n{error}\n\nStderr:\n{stderr}"
        
        # Feedback toutes les 10s
        if i % 10 == 0 and i > 0:
            print(f"‚è≥ Still waiting for job {job_id[:8]}... ({i}s elapsed)")
        
        time.sleep(1)
    
    return f"‚è±Ô∏è Timeout: Job {job_id} took longer than {timeout}s"


# ==================== TOOLS ====================

@mcp.tool()
def build_and_test_image(repo_url: str, dockerfile_content: str, tag: str) -> str:
    """
    Clone un repo, √©crit un Dockerfile, build l'image avec Buildah (rootless)
    et tente un dry-run pour v√©rifier les d√©pendances.
    
    Args:
        repo_url: URL du repo Git √† cloner
        dockerfile_content: Contenu du Dockerfile √† cr√©er
        tag: Tag de l'image (ex: 'mamba-jetson:v1')
    
    Returns:
        Build logs et r√©sultat du dry-run
    """
    return submit_job(
        "podman_build",
        repo_url=repo_url,
        dockerfile_content=dockerfile_content,
        tag=tag
    )


@mcp.tool()
def run_benchmark_in_container(image_tag: str, command: str, gpus: int = 1) -> str:
    """
    Lance une commande dans un container avec acc√®s GPU.
    
    Args:
        image_tag: Tag de l'image √† utiliser
        command: Commande √† ex√©cuter (ex: 'python train.py --epochs 1')
        gpus: Nombre de GPUs √† allouer (default: 1)
    
    Returns:
        Stdout de la commande
    """
    return submit_job(
        "podman_run",
        image_tag=image_tag,
        command=command,
        gpus=gpus
    )


@mcp.tool()
def run_script_on_hpc(script: str, partition: str = "dev", cpus: int = 8, 
                      mem: str = "64G", gpus: int = 1) -> str:
    """
    Ex√©cute un script bash arbitraire sur le HPC via srun.
    
    Args:
        script: Script bash √† ex√©cuter
        partition: Partition SLURM (default: 'dev')
        cpus: Nombre de CPUs (default: 8)
        mem: M√©moire (default: '64G')
        gpus: Nombre de GPUs (default: 1)
    
    Returns:
        Output du script
    """
    return submit_job(
        "srun_script",
        script=script,
        partition=partition,
        cpus=cpus,
        mem=mem,
        gpus=gpus
    )


@mcp.tool()
def check_huggingface_model(model_id: str) -> str:
    """
    Interroge l'API HuggingFace pour r√©cup√©rer les infos d'un mod√®le.
    
    Args:
        model_id: ID du mod√®le (ex: 'meta-llama/Llama-3.2-1B')
    
    Returns:
        Infos JSON du mod√®le (taille, safetensors, etc.)
    """
    return submit_job(
        "huggingface_check",
        model_id=model_id
    )


@mcp.tool()
def check_slurm_queue() -> str:
    """
    Affiche l'√©tat de la queue SLURM (squeue).
    
    Returns:
        Output de squeue format√©
    """
    return submit_job("slurm_queue")


@mcp.tool()
def get_gpu_info() -> str:
    """
    R√©cup√®re les infos des GPUs disponibles (nvidia-smi).
    
    Returns:
        Output de nvidia-smi
    """
    return submit_job("gpu_info")


if __name__ == "__main__":
    mcp.run()